{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d59ce9",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Tutorial, chapter 7\n",
    "\n",
    "In this tutorial you will learn how to\n",
    "\n",
    "- Convert and import the ``sfu-hw-objects-v1`` custom video dataset\n",
    "- Visualize frames from the video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37649d7",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37946/1348678174.py:6: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, Markdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://nbconvert.readthedocs.io/en/latest/removing_cells.html\n",
    "# use these magic spells to update your classes methods on-the-fly as you edit them:\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "# %run includeme.ipynb # include a notebook from this same directory\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef558334",
   "metadata": {},
   "source": [
    "In this tutorial you will learn how to:\n",
    "\n",
    "- Download and register video datasets\n",
    "- Convert and import the ``sfu-hw-objects-v1`` raw custom video data format\n",
    "- Play around with video datasets, visualize frames and detection results\n",
    "- Evaluate a video dataset\n",
    "\n",
    "In chapter 2 of this tutorial you learned how to download and register datasets to fiftyone with the ``compressai-vision register`` command.\n",
    "\n",
    "Exactly the same command works for video datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b7ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "\n",
      "WARNING: downloading ALL images.  You might want to use the --lists option to download only certain images\n",
      "Using list files:     None\n",
      "Number of images:     ?\n",
      "Database name   :     quickstart-video\n",
      "Subname/split   :     None\n",
      "Target dir      :     None\n",
      "\n",
      "Dataset already downloaded\n",
      "Loading existing dataset 'quickstart-video'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision download --dataset-name=quickstart-video --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f51728",
   "metadata": {},
   "source": [
    "If you have your video dataset arranged in one of the standard [video data formats supported by fiftyone](https://voxel51.com/docs/fiftyone/api/fiftyone.types.dataset_types.html), you're good to go.\n",
    "\n",
    "Manipulating and visualizing video datasets from python works a bit different to image datasets.  For this, please see the end of this tutorial.\n",
    "\n",
    "Next we will import a raw custom dataset, namely the [sfu-hw-objects-v1](http://dx.doi.org/10.17632/hwm673bv4m.1) into fiftyone.\n",
    "\n",
    "This format consists raw YUV video files and annotations.  Let's see how the folder structure is roughly organized.  We'll be using in this tutorial a \"mock\" version of the dataset with only two video classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6755a591",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "path_to_sfu_hw_objects_v1=\"/home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5483eb3",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "!find {path_to_sfu_hw_objects_v1} -name \"*.mkv\" | xargs -I + rm +\n",
    "!find {path_to_sfu_hw_objects_v1} -name \"*.webm\" | xargs -I + rm +\n",
    "!find {path_to_sfu_hw_objects_v1} -name \"*.mp4\" | xargs -I + rm +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9539b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1\r\n",
      "├── ClassC\r\n",
      "│   ├── Annotations\r\n",
      "│   │   └── BasketballDrill [502 entries exceeds filelimit, not opening dir]\r\n",
      "│   └── BasketballDrill_832x480_50Hz_8bit_P420.yuv\r\n",
      "└── ClassX\r\n",
      "    ├── Annotations\r\n",
      "    │   └── BasketballDrill\r\n",
      "    │       ├── BasketballDrill_832x480_50_seq_001.txt\r\n",
      "    │       ├── BasketballDrill_832x480_50_seq_002.txt\r\n",
      "    │       ├── BasketballDrill_832x480_50_seq_003.txt\r\n",
      "    │       ├── BasketballDrill_832x480_50_seq_004.txt\r\n",
      "    │       └── BasketballDrill_832x480_object.list\r\n",
      "    └── BasketballDrill_832x480_50Hz_8bit_P420.yuv -> /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassC/BasketballDrill_832x480_50Hz_8bit_P420.yuv\r\n",
      "\r\n",
      "6 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree {path_to_sfu_hw_objects_v1} --filelimit=10 | cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90165d2",
   "metadata": {},
   "source": [
    "Importing custom dataset can be done with ``import-custom`` command.  For ``sfu-hw-objects-v1`` it also converts on-the-fly the raw YUV images into proper video format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "227144a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "WARNING: dataset sfu-hw-objects-v1 already exists: will delete and rewrite\n",
      "\n",
      "Importing a custom video format into fiftyone\n",
      "\n",
      "Dataset type           :  sfu-hw-objects-v1\n",
      "Dataset root directory :  /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1\n",
      "\n",
      "finding .yuv files from /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1\n",
      "\n",
      "WARNING: converted video file /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassC/Annotations/BasketballDrill/video.mp4 exists already.  Will skip conversion.  Remove manually if needed\n",
      "\n",
      "\n",
      "WARNING: converted video file /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassX/Annotations/BasketballDrill/video.mp4 exists already.  Will skip conversion.  Remove manually if needed\n",
      "\n",
      "video conversion done\n",
      "searching for /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/Class*\n",
      "Dataset sfu-hw-objects-v1 exists.  Will remove it first\n",
      "Dataset sfu-hw-objects-v1 created\n",
      "\n",
      "In class directory /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassC\n",
      "searching for /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassC/Annotations/*\n",
      "--> registering video /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassC/Annotations/BasketballDrill/video.mp4\n",
      "--> registered new video sample: ClassC BasketballDrill with 500 frames\n",
      "\n",
      "In class directory /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassX\n",
      "searching for /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassX/Annotations/*\n",
      "--> registering video /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassX/Annotations/BasketballDrill/video.mp4\n",
      "--> registered new video sample: ClassX BasketballDrill with 4 frames\n",
      "\n",
      "Dataset saved\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision import-custom --dataset-type=sfu-hw-objects-v1 --dir={path_to_sfu_hw_objects_v1} --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe21fa",
   "metadata": {},
   "source": [
    "In order to demonstrate how video datasets are used, let's continue in python notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9ac82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436068de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=fo.load_dataset(\"sfu-hw-objects-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4dfd18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        sfu-hw-objects-v1\n",
       "Media type:  video\n",
       "Num samples: 2\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:         fiftyone.core.fields.ObjectIdField\n",
       "    filepath:   fiftyone.core.fields.StringField\n",
       "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.VideoMetadata)\n",
       "    media_type: fiftyone.core.fields.StringField\n",
       "    class_tag:  fiftyone.core.fields.StringField\n",
       "    name_tag:   fiftyone.core.fields.StringField\n",
       "    custom_id:  fiftyone.core.fields.StringField\n",
       "Frame fields:\n",
       "    id:           fiftyone.core.fields.ObjectIdField\n",
       "    frame_number: fiftyone.core.fields.FrameNumberField\n",
       "    detections:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63430a82",
   "metadata": {},
   "source": [
    "In contrast to image datasets where each sample was an image, now a sample corresponds to a video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c98e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '636cfcae2a92080b7e1490b5',\n",
       "    'media_type': 'video',\n",
       "    'filepath': '/home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassC/Annotations/BasketballDrill/video.mp4',\n",
       "    'tags': BaseList([]),\n",
       "    'metadata': None,\n",
       "    'class_tag': 'ClassC',\n",
       "    'name_tag': 'BasketballDrill',\n",
       "    'custom_id': 'ClassC_BasketballDrill',\n",
       "    'frames': <Frames: 500>,\n",
       "}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d35b41",
   "metadata": {},
   "source": [
    "There is a reference to the video file and a ``Frames`` object, encapsulating ground truths etc. data for each and every frame.  For ``sfu-hw-objects-v1`` in particular, ``class_tag`` corresponds to the class directories (ClassA, ClassB, etc.), while ``name_tag`` to the video descriptive names (BasketballDrill, Traffic, PeopleOnStreeet, etc.).  Let's pick a certain video sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7956d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[ (F(\"name_tag\") == \"BasketballDrill\") & (F(\"class_tag\") == \"ClassC\") ].first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac06cce",
   "metadata": {},
   "source": [
    "Take a look at the first frame ground truth detections (note that frame indices start from 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "536feb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FrameView: {\n",
       "    'id': '636cfcaf2c777ac16190f9bd',\n",
       "    'frame_number': 1,\n",
       "    'detections': <Detections: {\n",
       "        'detections': BaseList([\n",
       "            <Detection: {\n",
       "                'id': '636cfcad2a92080b7e147d04',\n",
       "                'attributes': BaseDict({}),\n",
       "                'tags': BaseList([]),\n",
       "                'label': 'person',\n",
       "                'bounding_box': BaseList([0.2525, 0.8288, 0.1812, 0.1678]),\n",
       "                'mask': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': None,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '636cfcad2a92080b7e147d05',\n",
       "                'attributes': BaseDict({}),\n",
       "                'tags': BaseList([]),\n",
       "                'label': 'person',\n",
       "                'bounding_box': BaseList([0.63635, 0.00874999999999998, 0.1207, 0.3149]),\n",
       "                'mask': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': None,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '636cfcad2a92080b7e147d06',\n",
       "                'attributes': BaseDict({}),\n",
       "                'tags': BaseList([]),\n",
       "                'label': 'person',\n",
       "                'bounding_box': BaseList([\n",
       "                    0.30820000000000003,\n",
       "                    0.32125000000000004,\n",
       "                    0.1828,\n",
       "                    0.5125,\n",
       "                ]),\n",
       "                'mask': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': None,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '636cfcad2a92080b7e147d07',\n",
       "                'attributes': BaseDict({}),\n",
       "                'tags': BaseList([]),\n",
       "                'label': 'person',\n",
       "                'bounding_box': BaseList([0.5392, 0.7257, 0.2042, 0.2812]),\n",
       "                'mask': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': None,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '636cfcad2a92080b7e147d08',\n",
       "                'attributes': BaseDict({}),\n",
       "                'tags': BaseList([]),\n",
       "                'label': 'sports ball',\n",
       "                'bounding_box': BaseList([\n",
       "                    0.045313000000000006,\n",
       "                    0.37777800000000006,\n",
       "                    0.160156,\n",
       "                    0.2375,\n",
       "                ]),\n",
       "                'mask': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': None,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '636cfcad2a92080b7e147d09',\n",
       "                'attributes': BaseDict({}),\n",
       "                'tags': BaseList([]),\n",
       "                'label': 'sports ball',\n",
       "                'bounding_box': BaseList([\n",
       "                    0.142969,\n",
       "                    0.020833499999999998,\n",
       "                    0.03125,\n",
       "                    0.061111,\n",
       "                ]),\n",
       "                'mask': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': None,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '636cfcad2a92080b7e147d0a',\n",
       "                'attributes': BaseDict({}),\n",
       "                'tags': BaseList([]),\n",
       "                'label': 'chair',\n",
       "                'bounding_box': BaseList([\n",
       "                    0.11015650000000002,\n",
       "                    0.002777500000000002,\n",
       "                    0.096875,\n",
       "                    0.176389,\n",
       "                ]),\n",
       "                'mask': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': None,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '636cfcad2a92080b7e147d0b',\n",
       "                'attributes': BaseDict({}),\n",
       "                'tags': BaseList([]),\n",
       "                'label': 'chair',\n",
       "                'bounding_box': BaseList([\n",
       "                    0.18125000000000002,\n",
       "                    0.0041665000000000035,\n",
       "                    0.089062,\n",
       "                    0.141667,\n",
       "                ]),\n",
       "                'mask': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': None,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '636cfcad2a92080b7e147d0c',\n",
       "                'attributes': BaseDict({}),\n",
       "                'tags': BaseList([]),\n",
       "                'label': 'chair',\n",
       "                'bounding_box': BaseList([\n",
       "                    0.2460935,\n",
       "                    0.0013889999999999944,\n",
       "                    0.082031,\n",
       "                    0.115278,\n",
       "                ]),\n",
       "                'mask': None,\n",
       "                'confidence': 1.0,\n",
       "                'index': None,\n",
       "            }>,\n",
       "        ]),\n",
       "    }>,\n",
       "}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.frames[1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64fc4d6d",
   "metadata": {},
   "source": [
    "Start reading the video file with OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9381b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid=cv2.VideoCapture(sample.filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f2ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of frames: 501\n"
     ]
    }
   ],
   "source": [
    "print(\"number of frames:\",int(vid.get(cv2.CAP_PROP_FRAME_COUNT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99744f",
   "metadata": {},
   "source": [
    "Let's define a small helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1859d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections(sample: fo.Sample, vid: cv2.VideoCapture, nframe: int):\n",
    "    nmax=int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"nmax\", nmax)\n",
    "    if nframe > nmax:\n",
    "        raise AssertionError(\"max frame is \" + str(nmax))\n",
    "    ok = vid.set(cv2.CAP_PROP_POS_FRAMES, nframe-1)\n",
    "    if not ok:\n",
    "        raise AssertionError(\"seek failed\")\n",
    "    ok, arr = vid.read() # BGR image in arr\n",
    "    if not ok:\n",
    "        raise AssertionError(\"no image\")\n",
    "    for detection in sample.frames[nframe].detections.detections:\n",
    "        x0, y0, w, h = detection.bounding_box # rel coords\n",
    "        print(x0, y0, w, h)\n",
    "        x1, y1, x2, y2 = floor(x0*arr.shape[1]), floor(y0*arr.shape[0]), floor((x0+w)*arr.shape[1]), floor((y0+h)*arr.shape[0])\n",
    "        arr=cv2.rectangle(arr, (x1, y1), (x2, y2), (255, 0, 0), 5)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a254716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmax 501\n",
      "0.2244 0.21975 0.1462 0.2045\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'floor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img\u001b[38;5;241m=\u001b[39m\u001b[43mdraw_detections\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m img_ \u001b[38;5;241m=\u001b[39m img[:,:,::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mdraw_detections\u001b[0;34m(sample, vid, nframe)\u001b[0m\n\u001b[1;32m     13\u001b[0m     x0, y0, w, h \u001b[38;5;241m=\u001b[39m detection\u001b[38;5;241m.\u001b[39mbounding_box \u001b[38;5;66;03m# rel coords\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x0, y0, w, h)\n\u001b[0;32m---> 15\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[43mfloor\u001b[49m(x0\u001b[38;5;241m*\u001b[39marr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), floor(y0\u001b[38;5;241m*\u001b[39marr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), floor((x0\u001b[38;5;241m+\u001b[39mw)\u001b[38;5;241m*\u001b[39marr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), floor((y0\u001b[38;5;241m+\u001b[39mh)\u001b[38;5;241m*\u001b[39marr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     16\u001b[0m     arr\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mrectangle(arr, (x1, y1), (x2, y2), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "\u001b[0;31mNameError\u001b[0m: name 'floor' is not defined"
     ]
    }
   ],
   "source": [
    "img=draw_detections(sample, vid, 200)\n",
    "img_ = img[:,:,::-1] # BGR -> RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67413cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_)\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647da96a",
   "metadata": {},
   "source": [
    "Visualize video and annotations in the fiftyone app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7be1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342d2db",
   "metadata": {},
   "source": [
    "In chapters 3 and 4 you learned how to evaluate models (in serial and parallel) with the ``compressai-vision detectron2-eval`` command.\n",
    "\n",
    "The same command can be used to evaluate video datasets as well.  Here the parameter ``--slice`` refers to videos, not individual image (as usual, for a production run, you would remove the ``--slice`` parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!compressai-vision detectron2-eval --y --dataset-name=sfu-hw-objects-v1 \\\n",
    "--slice=1:2 \\\n",
    "--scale=100 \\\n",
    "--progressbar \\\n",
    "--output=detectron2_test.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a58d3",
   "metadata": {},
   "source": [
    "Take a look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat detectron2_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688beeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
