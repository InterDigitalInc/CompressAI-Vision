{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d307e898",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Tutorial, chapter 3\n",
    "\n",
    "\n",
    "- mpeg-vcm-auto-import\n",
    "- run evaluations for the mpeg-vcm dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb280563",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53288/1348678174.py:6: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, Markdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://nbconvert.readthedocs.io/en/latest/removing_cells.html\n",
    "# use these magic spells to update your classes methods on-the-fly as you edit them:\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "# %run includeme.ipynb # include a notebook from this same directory\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706b51d",
   "metadata": {},
   "source": [
    "In this chapter you will learn:\n",
    "\n",
    "- to import the mpeg-vcm working-group custom datasets\n",
    "- running evaluation on dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726bdb7f",
   "metadata": {},
   "source": [
    "The mpeg-vcm working group defines several custom datasets for evaluating the performance of your deep-learning de/compression algorithm.  For more details, please see [here](file:///home/sampsa/silo/interdigital/CompressAI-Vision/docs/_build/html/datasets.html).\n",
    "\n",
    "The tricky part is importing all that data into fiftyone.  Once we have done that, we can use the CLI tools to evaluate the de/compression model with the mpeg-vcm defined pipeline, i.e.:\n",
    "```\n",
    "mpeg-vcm custom dataset --> compression and decompression --> Detectron2 predictor --> mAP\n",
    "```\n",
    "All the datasets can be download and/or registered into fiftyone with the ``compressai-vision import-custom`` command.\n",
    "\n",
    "For example, after running ``compressai-vision import-custom oiv6-mpeg-v1`` you will have the following datasets:\n",
    "\n",
    "- ``oiv6-mpeg-detection-v1``\n",
    "- ``oiv6-mpeg-segmentation-v1``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4f013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "\n",
      "datasets currently registered into fiftyone\n",
      "name, length, first sample path\n",
      "flir-image-rgb-v1, 10318, /media/sampsa/4d0dff98-8e61-4a0b-a97e-ceb6bc7ccb4b/datasets/flir/images_rgb_train/data\n",
      "oiv6-mpeg-detection-v1, 5000, /home/sampsa/fiftyone/oiv6-mpeg-detection-v1/data\n",
      "oiv6-mpeg-detection-v1-dummy, 1, /home/sampsa/fiftyone/oiv6-mpeg-detection-v1/data\n",
      "oiv6-mpeg-segmentation-v1, 5000, /home/sampsa/fiftyone/oiv6-mpeg-segmentation-v1/data\n",
      "open-images-v6-validation, 8189, /home/sampsa/fiftyone/open-images-v6/validation/data\n",
      "quickstart, 200, /home/sampsa/fiftyone/quickstart/data\n",
      "quickstart-video, 10, /home/sampsa/fiftyone/quickstart-video/data\n",
      "sfu-hw-objects-v1, 2, /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassC/Annotations/BasketballDrill\n",
      "tvd-image-detection-v1, 167, /media/sampsa/4d0dff98-8e61-4a0b-a97e-ceb6bc7ccb4b/datasets/tvd/TVD_images_detection_v1/data\n",
      "tvd-image-segmentation-v1, 167, /media/sampsa/4d0dff98-8e61-4a0b-a97e-ceb6bc7ccb4b/datasets/tvd/TVD_images_segmentation_v1/data\n",
      "tvd-object-tracking-v1, 3, /media/sampsa/4d0dff98-8e61-4a0b-a97e-ceb6bc7ccb4b/datasets/tvd/TVD_object_tracking_dataset_and_annotations\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1da9a8",
   "metadata": {},
   "source": [
    "Now we can continue by evaluating the datasets agains a compressai model, like we did in chapter 1.  Before that, let's take a closer look at the dataset ``oiv6-mpeg-detection-v1``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f428b2",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "\n",
      "dataset info:\n",
      "Name:        oiv6-mpeg-detection-v1\n",
      "Media type:  image\n",
      "Num samples: 5000\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:              fiftyone.core.fields.ObjectIdField\n",
      "    filepath:        fiftyone.core.fields.StringField\n",
      "    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    positive_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    negative_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    detections:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    open_images_id:  fiftyone.core.fields.StringField\n",
      "\n",
      "test-loading first image from /home/sampsa/fiftyone/oiv6-mpeg-detection-v1/data/0001eeaf4aed83f9.jpg\n",
      "loaded image with dimensions (447, 1024, 3) ok\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision show --dataset-name=oiv6-mpeg-detection-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccbe42",
   "metadata": {},
   "source": [
    "Detection data ground truths (bounding boxes) in each sample are in the field ``detections``, so we need to use ``--gt-field=detections``.  Evaluation method for mAP is the OpenImagesV6 protocol, so we use ``--eval-method=open-images``.  For a quick test run we just run the evaluation with the two first images of the dataset with ``--slice=0:2`` (for an actual production run, remove it).  \n",
    "\n",
    "To get an mAP reference value (without any sort of de/compression), we run crunch images through a Detectron2 predictor and compare to the ground truths in field ``detections``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2975495a",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "WARNING: using a dataset slice instead of full dataset: SURE YOU WANT THIS?\n",
      "instantiating Detectron2 predictor 0 : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
      "\n",
      "Using dataset          : oiv6-mpeg-detection-v1\n",
      "Dataset media type     : image\n",
      "Dataset tmp clone      : detectron-run-sampsa-oiv6-mpeg-detection-v1-2022-11-16-17-21-51-787050\n",
      "Keep tmp dataset?      : False\n",
      "Image scaling          : 100\n",
      "WARNING: Using slice   : 0:2\n",
      "Number of samples      : 2\n",
      "Torch device           : cpu\n",
      "=== Vision Model #0 ====\n",
      "Detectron2 model       : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
      "Model was trained with : coco_2017_train\n",
      "Eval. results will be saved to datafield\n",
      "                       : detectron-predictions_v0\n",
      "Evaluation protocol    : open-images\n",
      "Peek model classes     :\n",
      "['airplane', 'apple', 'backpack', 'banana', 'baseball bat'] ...\n",
      "Peek dataset classes   :\n",
      "['airplane', 'person'] ...\n",
      "** Evaluation without Encoding/Decoding **\n",
      "Ground truth data field name\n",
      "                       : detections\n",
      "Progressbar            : True\n",
      "WARNING: progressbar enabled --> disabling normal progress print\n",
      "Print progress         : 0\n",
      "Output file            : detectron2_mpeg_vcm.json\n",
      "cloning dataset oiv6-mpeg-detection-v1 to detectron-run-sampsa-oiv6-mpeg-detection-v1-2022-11-16-17-21-51-787050\n",
      "/home/sampsa/silo/interdigital/venv_all/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 Evaluating detections...\n",
      " 100% |███████████| 2/2 [24.9ms elapsed, 0s remaining, 80.3 samples/s] \n",
      "deleting tmp database detectron-run-sampsa-oiv6-mpeg-detection-v1-2022-11-16-17-21-51-787050\n",
      "\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision detectron2-eval --y --dataset-name=oiv6-mpeg-detection-v1 \\\n",
    "--slice=0:2 \\\n",
    "--gt-field=detections \\\n",
    "--eval-method=open-images \\\n",
    "--progressbar \\\n",
    "--output=detectron2_mpeg_vcm.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f9b05",
   "metadata": {},
   "source": [
    "Next we create two points on the mAP(bbp) curve for the compressai pre-trained ``bmshj2018_factorized`` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ee9d531",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "WARNING: using a dataset slice instead of full dataset: SURE YOU WANT THIS?\n",
      "instantiating Detectron2 predictor 0 : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
      "\n",
      "Using dataset          : oiv6-mpeg-detection-v1\n",
      "Dataset media type     : image\n",
      "Dataset tmp clone      : detectron-run-sampsa-oiv6-mpeg-detection-v1-2022-11-16-17-28-02-372323\n",
      "Keep tmp dataset?      : False\n",
      "Image scaling          : 100\n",
      "WARNING: Using slice   : 0:2\n",
      "Number of samples      : 2\n",
      "Torch device           : cpu\n",
      "=== Vision Model #0 ====\n",
      "Detectron2 model       : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
      "Model was trained with : coco_2017_train\n",
      "Eval. results will be saved to datafield\n",
      "                       : detectron-predictions_v0\n",
      "Evaluation protocol    : open-images\n",
      "Peek model classes     :\n",
      "['airplane', 'apple', 'backpack', 'banana', 'baseball bat'] ...\n",
      "Peek dataset classes   :\n",
      "['airplane', 'person'] ...\n",
      "Using compressai model : bmshj2018-factorized\n",
      "Quality parameters     : [1, 2]\n",
      "Ground truth data field name\n",
      "                       : detections\n",
      "Progressbar            : True\n",
      "WARNING: progressbar enabled --> disabling normal progress print\n",
      "Print progress         : 0\n",
      "Output file            : detectron2_mpeg_vcm_qpars.json\n",
      "cloning dataset oiv6-mpeg-detection-v1 to detectron-run-sampsa-oiv6-mpeg-detection-v1-2022-11-16-17-28-02-372323\n",
      "\n",
      "QUALITY PARAMETER:  1\n",
      "/home/sampsa/silo/interdigital/venv_all/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 Evaluating detections...\n",
      " 100% |███████████| 2/2 [15.2ms elapsed, 0s remaining, 131.9 samples/s] \n",
      "\n",
      "QUALITY PARAMETER:  2\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 Evaluating detections...\n",
      " 100% |███████████| 2/2 [21.9ms elapsed, 0s remaining, 91.4 samples/s] \n",
      "deleting tmp database detectron-run-sampsa-oiv6-mpeg-detection-v1-2022-11-16-17-28-02-372323\n",
      "\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision detectron2-eval --y --dataset-name=oiv6-mpeg-detection-v1 \\\n",
    "--slice=0:2 \\\n",
    "--gt-field=detections \\\n",
    "--eval-method=open-images \\\n",
    "--progressbar \\\n",
    "--qpars=1,2 \\\n",
    "--compressai-model-name=bmshj2018-factorized \\\n",
    "--output=detectron2_mpeg_vcm_qpars.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fb95e",
   "metadata": {},
   "source": [
    "Again, for an actual production run, you would remove the ``--slice`` argument.  You can run all quality points (bpp values) in a single run, say by defining ``--qpars=1,2,3,4,5,6,7,8``, or if you want to parallelize, send the same command to your queue system several times, each time with a different quality parameter values, i.e. ``--qpars=1``, ``--qpars=2``, etc.\n",
    "\n",
    "Again, and as explained in tutorial 1 you can visualize your dataset with ``compressai-vision app`` command and compare ground-truths and detections if you use ``--keep`` flag with the ``detectron2-eval`` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd163a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
