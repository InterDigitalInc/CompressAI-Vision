{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d307e898",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Tutorial, chapter 3\n",
    "\n",
    "\n",
    "- mpeg-vcm-auto-import\n",
    "- run evaluations for the mpeg-vcm dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb280563",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33839/1348678174.py:6: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, Markdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://nbconvert.readthedocs.io/en/latest/removing_cells.html\n",
    "# use these magic spells to update your classes methods on-the-fly as you edit them:\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "# %run includeme.ipynb # include a notebook from this same directory\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706b51d",
   "metadata": {},
   "source": [
    "In this chapter you will learn:\n",
    "\n",
    "- to import the mpeg-vcm datasets (from the mpeg-vcm working group)\n",
    "- running evaluation on the mpeg-vcm datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726bdb7f",
   "metadata": {},
   "source": [
    "The mpeg-vcm working group defines a specially prepared custom datasets (subset of OpenImageV6) for evaluating the performance of your deep-learning de/compression algorithm.\n",
    "\n",
    "The tricky part is importing all that data into fiftyone.  Once we have done that, we can use the CLI tools to evaluate the de/compression model with the mpeg-vcm defined pipeline, i.e.:\n",
    "```\n",
    "mpeg-vcm custom dataset --> compression and decompression --> Detectron2 predictor --> mAP\n",
    "```\n",
    "\n",
    "The CLI tools have a subcommand ``mpeg-vcm-auto-import`` that downloads necessary images from the open images dataset and prepares the annotations according to mpeg-vcm working group specifications, so the only thing you need to do to get started, is simply to type\n",
    "```\n",
    "compressai-vision mpeg-vcm-auto-import\n",
    "```\n",
    "After running that \"wizard\", you should have the mpeg-vcm datasets registered into fiftyone (``mpeg-vcm-detection`` etc. datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4f013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "\n",
      "datasets currently registered into fiftyone\n",
      "name, length, first sample path\n",
      "mpeg-vcm-detection, 5000, /home/sampsa/fiftyone/mpeg-vcm-detection/data\n",
      "mpeg-vcm-detection-dummy, 1, /home/sampsa/fiftyone/mpeg-vcm-detection/data\n",
      "mpeg-vcm-segmentation, 5000, /home/sampsa/fiftyone/mpeg-vcm-segmentation/data\n",
      "open-images-v6-validation, 8189, /home/sampsa/fiftyone/open-images-v6/validation/data\n",
      "quickstart, 200, /home/sampsa/fiftyone/quickstart/data\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1da9a8",
   "metadata": {},
   "source": [
    "Now we can continue by evaluating the datasets agains a compressai model, like we did in chapter 1.  Before that, let's take a closer look at the dataset ``mpeg-vcm-detection``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f428b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "\n",
      "dataset info:\n",
      "Name:        mpeg-vcm-detection\n",
      "Media type:  image\n",
      "Num samples: 5000\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:              fiftyone.core.fields.ObjectIdField\n",
      "    filepath:        fiftyone.core.fields.StringField\n",
      "    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    positive_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    negative_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
      "    detections:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    open_images_id:  fiftyone.core.fields.StringField\n",
      "\n",
      "test-loading first image from /home/sampsa/fiftyone/mpeg-vcm-detection/data/0001eeaf4aed83f9.jpg\n",
      "loaded image with dimensions (447, 1024, 3) ok\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision show --dataset-name=mpeg-vcm-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccbe42",
   "metadata": {},
   "source": [
    "Detection data ground truths (bounding boxes) in each sample are in the field ``detections``, so we need to use ``--gt-field=detections``.  Evaluation method for mAP is the OpenImagesV6 protocol, so we use ``--eval-method=open-images``.  For a quick test run we just run the evaluation with the two first images of the dataset with ``--slice=0:2`` (for an actual production run, remove it).  \n",
    "\n",
    "To get an mAP reference value (without any sort of de/compression), we run crunch images through a Detectron2 predictor and compare to the ground truths in field ``detections``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2975495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "WARNING: using a dataset slice instead of full dataset: SURE YOU WANT THIS?\n",
      "\n",
      "Using dataset          : mpeg-vcm-detection\n",
      "Dataset tmp clone      : detectron-run-sampsa-mpeg-vcm-detection-2022-10-07-16-10-22-138077\n",
      "Image scaling          : 100\n",
      "WARNING: Using slice   : 0:2\n",
      "Number of samples      : 2\n",
      "Torch device           : cpu\n",
      "Detectron2 model       : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
      "Model was trained with : coco_2017_train\n",
      "** Evaluation without Encoding/Decoding **\n",
      "Ground truth data field name\n",
      "                       : detections\n",
      "Eval. results will be saved to datafield\n",
      "                       : detectron-predictions\n",
      "Evaluation protocol    : open-images\n",
      "Progressbar            : True\n",
      "WARNING: progressbar enabled --> disabling normal progress print\n",
      "Print progress         : 0\n",
      "Output file            : detectron2_mpeg_vcm.json\n",
      "Peek model classes     :\n",
      "['airplane', 'apple', 'backpack', 'banana', 'baseball bat'] ...\n",
      "Peek dataset classes   :\n",
      "['airplane', 'person'] ...\n",
      "cloning dataset mpeg-vcm-detection to detectron-run-sampsa-mpeg-vcm-detection-2022-10-07-16-10-22-138077\n",
      "instantiating Detectron2 predictor\n",
      "/home/sampsa/silo/interdigital/venv_all/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 error: number of pixels sum < 1\n",
      "Ignoring unsupported parameters {'compute_mAP'} for <class 'fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig'>\n",
      "Evaluating detections...\n",
      " 100% |███████████| 2/2 [38.8ms elapsed, 0s remaining, 51.5 samples/s] \n",
      "deleting tmp database detectron-run-sampsa-mpeg-vcm-detection-2022-10-07-16-10-22-138077\n",
      "\n",
      "HAVE A NICE DAY!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision detectron2-eval --y --dataset-name=mpeg-vcm-detection \\\n",
    "--slice=0:2 \\\n",
    "--gt-field=detections \\\n",
    "--eval-method=open-images \\\n",
    "--progressbar \\\n",
    "--output=detectron2_mpeg_vcm.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f9b05",
   "metadata": {},
   "source": [
    "Next we create two points on the mAP(bbp) curve for the compressai pre-trained ``bmshj2018_factorized`` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee9d531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "WARNING: using a dataset slice instead of full dataset: SURE YOU WANT THIS?\n",
      "\n",
      "Using dataset          : mpeg-vcm-detection\n",
      "Dataset tmp clone      : detectron-run-sampsa-mpeg-vcm-detection-2022-10-07-16-17-12-468267\n",
      "Image scaling          : 100\n",
      "WARNING: Using slice   : 0:2\n",
      "Number of samples      : 2\n",
      "Torch device           : cpu\n",
      "Detectron2 model       : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
      "Model was trained with : coco_2017_train\n",
      "Using compressai model : bmshj2018_factorized\n",
      "Quality parameters     : [1, 2]\n",
      "Ground truth data field name\n",
      "                       : detections\n",
      "Eval. results will be saved to datafield\n",
      "                       : detectron-predictions\n",
      "Evaluation protocol    : open-images\n",
      "Progressbar            : True\n",
      "WARNING: progressbar enabled --> disabling normal progress print\n",
      "Print progress         : 0\n",
      "Output file            : detectron2_mpeg_vcm_qpars.json\n",
      "Peek model classes     :\n",
      "['airplane', 'apple', 'backpack', 'banana', 'baseball bat'] ...\n",
      "Peek dataset classes   :\n",
      "['airplane', 'person'] ...\n",
      "cloning dataset mpeg-vcm-detection to detectron-run-sampsa-mpeg-vcm-detection-2022-10-07-16-17-12-468267\n",
      "instantiating Detectron2 predictor\n",
      "\n",
      "QUALITY PARAMETER:  1\n",
      "/home/sampsa/silo/interdigital/venv_all/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 Ignoring unsupported parameters {'compute_mAP'} for <class 'fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig'>\n",
      "Evaluating detections...\n",
      " 100% |███████████| 2/2 [23.9ms elapsed, 0s remaining, 83.7 samples/s] \n",
      "\n",
      "QUALITY PARAMETER:  2\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 Ignoring unsupported parameters {'compute_mAP'} for <class 'fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig'>\n",
      "Evaluating detections...\n",
      " 100% |███████████| 2/2 [26.6ms elapsed, 0s remaining, 75.2 samples/s] \n",
      "deleting tmp database detectron-run-sampsa-mpeg-vcm-detection-2022-10-07-16-17-12-468267\n",
      "\n",
      "HAVE A NICE DAY!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision detectron2-eval --y --dataset-name=mpeg-vcm-detection \\\n",
    "--slice=0:2 \\\n",
    "--gt-field=detections \\\n",
    "--eval-method=open-images \\\n",
    "--progressbar \\\n",
    "--qpars=1,2 \\\n",
    "--compressai-model-name=bmshj2018_factorized \\\n",
    "--output=detectron2_mpeg_vcm_qpars.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fb95e",
   "metadata": {},
   "source": [
    "Again, for an actual production run, you would remove the ``--slice`` argument.  You can run all quality points (bpp values) in a single run, say by defining ``--qpars=1,2,3,4,5,6,7,8``, or if you want to parallelize, send the same command to your queue system several times, each time with a different quality parameter values, i.e. ``--qpars=1``, ``--qpars=2``, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd163a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
