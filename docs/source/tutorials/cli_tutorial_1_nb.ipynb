{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb4c91d",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26495/1348678174.py:6: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, Markdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://nbconvert.readthedocs.io/en/latest/removing_cells.html\n",
    "# use these magic spells to update your classes methods on-the-fly as you edit them:\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "# %run includeme.ipynb # include a notebook from this same directory\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578782b",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "!compressai-vision clean\n",
    "!compressai-vision deregister --dataset-name quickstart-2-dummy --y\n",
    "!compressai-vision deregister --dataset-name quickstart --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7ec4a",
   "metadata": {},
   "source": [
    "The command line interface (cli) has all the functionality for evaluating your deep-learning compression algorithm against standardized benchmars.\n",
    "\n",
    "The cli is accessed with the ``compressai-vision`` command that has several subcommands for handling datasets, evaluating your models with them and for generating plots.  In detail:\n",
    "\n",
    "- ``compressai-vision -h`` gives you a short description of all commands\n",
    "- ``compressai-vision manual`` shows you a more thorough description\n",
    "- ``compressai-vision subcommand -h`` gives a detailed description of a certain subcommand\n",
    "\n",
    "The very first subcommand you should try is ``info``.  It gives you information about the installed software stack, library versions and registered datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd69b871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** TORCH, CUDA, DETECTRON2, COMPRESSAI ***\n",
      "torch version       : 1.9.1+cu102\n",
      "cuda version        : 10.2\n",
      "detectron2 version  : 0.6\n",
      "compressai version  : 1.2.0.dev0\n",
      "\n",
      "*** CHECKING GPU AVAILABILITY ***\n",
      "device              : cpu\n",
      "\n",
      "*** TESTING FFMPEG ***\n",
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "libavutil      56. 31.100 / 56. 31.100\n",
      "libavcodec     58. 54.100 / 58. 54.100\n",
      "libavformat    58. 29.100 / 58. 29.100\n",
      "libavdevice    58.  8.100 / 58.  8.100\n",
      "libavfilter     7. 57.100 /  7. 57.100\n",
      "libavresample   4.  0.  0 /  4.  0.  0\n",
      "libswscale      5.  5.100 /  5.  5.100\n",
      "libswresample   3.  5.100 /  3.  5.100\n",
      "libpostproc    55.  5.100 / 55.  5.100\n",
      "\n",
      "NOTICE: Using mongodb managed by fiftyone\n",
      "Be sure not to have extra mongod server(s) running on your system\n",
      "importing fiftyone..\n",
      "..imported\n",
      "fiftyone version: 0.16.6\n",
      "\n",
      "*** DATABASE ***\n",
      "info about your connection:\n",
      "Database(MongoClient(host=['localhost:34001'], document_class=dict, tz_aware=False, connect=True, appname='fiftyone'), 'fiftyone')\n",
      "\n",
      "\n",
      "*** DATASETS ***\n",
      "datasets currently registered into fiftyone\n",
      "name, length, first sample path\n",
      "mpeg-vcm-detection, 5000, /home/sampsa/fiftyone/mpeg-vcm-detection/data\n",
      "mpeg-vcm-detection-dummy, 1, /home/sampsa/fiftyone/mpeg-vcm-detection/data\n",
      "mpeg-vcm-segmentation, 5000, /home/sampsa/fiftyone/mpeg-vcm-segmentation/data\n",
      "open-images-v6-validation, 8189, /home/sampsa/fiftyone/open-images-v6/validation/data\n",
      "quickstart, 200, /home/sampsa/fiftyone/quickstart/data\n",
      "quickstart-2-dummy, 1, /tmp/kokkelis/quickstart/data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e600757",
   "metadata": {},
   "source": [
    "Another basic command is ``list`` that just shows you the registered datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87abd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "\n",
      "datasets currently registered into fiftyone\n",
      "name, length, first sample path\n",
      "mpeg-vcm-detection, 5000, /home/sampsa/fiftyone/mpeg-vcm-detection/data\n",
      "mpeg-vcm-detection-dummy, 1, /home/sampsa/fiftyone/mpeg-vcm-detection/data\n",
      "mpeg-vcm-segmentation, 5000, /home/sampsa/fiftyone/mpeg-vcm-segmentation/data\n",
      "open-images-v6-validation, 8189, /home/sampsa/fiftyone/open-images-v6/validation/data\n",
      "quickstart, 200, /home/sampsa/fiftyone/quickstart/data\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f23dc4",
   "metadata": {},
   "source": [
    "Datasets can be registered to and deregistered from fiftyone using the ``register`` and ``deregister`` subcommands, and downloaded and registered directly from [fiftyone dataset zoo](https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/datasets.html#dataset-zoo-quickstart) with the ``download`` command.  Let's use ``download`` to get the \"quickstart\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fff8adeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "\n",
      "WARNING: downloading ALL images.  You might want to use the --lists option to download only certain images\n",
      "Using list files:     None\n",
      "Number of images:     ?\n",
      "Database name   :     quickstart\n",
      "Subname/split   :     None\n",
      "Target dir      :     None\n",
      "\n",
      "Dataset already downloaded\n",
      "Loading existing dataset 'quickstart'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision download --dataset-name=quickstart --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3c53a",
   "metadata": {},
   "source": [
    "Nice, we have ourselves a dataset to play with.  A note: the ``--y`` switch makes the command to run in non-interactive mode.  Let's take a closer look at the fields that the samples have in this datafield with ``show``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cd2b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "\n",
      "Name:        quickstart\n",
      "Media type:  image\n",
      "Num samples: 200\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:           fiftyone.core.fields.ObjectIdField\n",
      "    filepath:     fiftyone.core.fields.StringField\n",
      "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    uniqueness:   fiftyone.core.fields.FloatField\n",
      "    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision show --dataset-name=quickstart --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3e468",
   "metadata": {},
   "source": [
    "Some fields of interests in each sample: ``filepath`` fields have the path to the downloaded images, while ``ground_truth`` fields have the ground-truth bounding boxes (\"quickstart\" dataset is a demo subset of COCO).\n",
    "\n",
    "Next we'll crunch all the images in the dataset through a Detectron2 predictor and evaluate the results using the COCO evaluation protocol: as a result, we'll get a mAP accuracy for the Detectron2 model.  Note that we have to indicate the ground truth field with ``--gt-field=ground_truth``.  Option ``--slice=0:2`` takes only the first two samples from the dataset for this run: its only for debugging run, so please feel free to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9922d0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "WARNING: using a dataset slice instead of full dataset\n",
      "SURE YOU WANT THIS?\n",
      "\n",
      "Using dataset          : quickstart\n",
      "Dataset tmp clone      : detectron-run-sampsa-quickstart-2022-10-04-22-04-02-138278\n",
      "Image scaling          : 100\n",
      "WARNING: Using slice   : 0:2\n",
      "Number of samples      : 2\n",
      "Torch device           : cpu\n",
      "Detectron2 model       : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
      "Model was trained with : coco_2017_train\n",
      "** Evaluation without Encoding/Decoding **\n",
      "Ground truth data field name\n",
      "                       : ground_truth\n",
      "Eval. results will be saved to datafield\n",
      "                       : detectron-predictions\n",
      "Evaluation protocol    : coco\n",
      "Progressbar            : True\n",
      "WARNING: progressbar enabled --> disabling normal progress print\n",
      "Print progress         : 0\n",
      "Output file            : detectron2_test.json\n",
      "Peek model classes     :\n",
      "['airplane', 'apple', 'backpack', 'banana', 'baseball bat'] ...\n",
      "Peek dataset classes   :\n",
      "['bird', 'horse', 'person'] ...\n",
      "cloning dataset quickstart to detectron-run-sampsa-quickstart-2022-10-04-22-04-02-138278\n",
      "instantiating Detectron2 predictor\n",
      "/home/sampsa/silo/interdigital/venv_all/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 error: number of pixels sum < 1\n",
      "Evaluating detections...\n",
      " 100% |███████████| 2/2 [9.7ms elapsed, 0s remaining, 206.0 samples/s] \n",
      "Performing IoU sweep...\n",
      " 100% |███████████| 2/2 [15.8ms elapsed, 0s remaining, 126.8 samples/s] \n",
      "deleting tmp database detectron-run-sampsa-quickstart-2022-10-04-22-04-02-138278\n",
      "\n",
      "HAVE A NICE DAY!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision detectron2-eval --y --dataset-name=quickstart \\\n",
    "--slice=0:2 \\\n",
    "--gt-field=ground_truth \\\n",
    "--eval-method=coco \\\n",
    "--progressbar \\\n",
    "--output=detectron2_test.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaea264",
   "metadata": {},
   "source": [
    "Let's see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8bd8a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"dataset\": \"quickstart\", \"gt_field\": \"ground_truth\", \"tmp datasetname\": \"detectron-run-sampsa-quickstart-2022-10-04-22-04-02-138278\", \"slice\": \"0:2\", \"model\": \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\", \"compressai model\": null, \"custom model\": null, \"checkpoint\": null, \"vtm\": false, \"vtm_cache\": null, \"qpars\": null, \"bpp\": [-1], \"map\": [0.5676567656765678], \"map_per_class\": [{\"bird\": 0.30297029702970296, \"horse\": 0.5, \"person\": 0.9}]}"
     ]
    }
   ],
   "source": [
    "!cat detectron2_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dba7f0",
   "metadata": {},
   "source": [
    "Now we use again a Detectron2 predictor on our dataset.   However, before passing the images to Detectron2 model, they are first compressed and decompressed by using a pre-trained compressai model with a quality parameter 1 (``--qpars=1``).\n",
    "\n",
    "We could evaluate for several quality parameters in serial by defining a list, i.e: ``--qpars=1,2,3`` and in parallel by launching the command separately for each particular value (say, for calculations in a queue/grid system).\n",
    "\n",
    "A scaling can be applied on the images, as defined by the mpeg-vcm specifications (``--scale=100``).  Again, remember to remove ``--slice=0:2`` for an actual run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49dfee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "WARNING: using a dataset slice instead of full dataset\n",
      "SURE YOU WANT THIS?\n",
      "\n",
      "Using dataset          : quickstart\n",
      "Dataset tmp clone      : detectron-run-sampsa-quickstart-2022-10-04-22-17-06-604353\n",
      "Image scaling          : 100\n",
      "WARNING: Using slice   : 0:2\n",
      "Number of samples      : 2\n",
      "Torch device           : cpu\n",
      "Detectron2 model       : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
      "Model was trained with : coco_2017_train\n",
      "Using compressai model : bmshj2018_factorized\n",
      "Quality parameters     : [1]\n",
      "Ground truth data field name\n",
      "                       : ground_truth\n",
      "Eval. results will be saved to datafield\n",
      "                       : detectron-predictions\n",
      "Evaluation protocol    : coco\n",
      "Progressbar            : True\n",
      "WARNING: progressbar enabled --> disabling normal progress print\n",
      "Print progress         : 0\n",
      "Output file            : compressai_detectron2_test.json\n",
      "Peek model classes     :\n",
      "['airplane', 'apple', 'backpack', 'banana', 'baseball bat'] ...\n",
      "Peek dataset classes   :\n",
      "['bird', 'horse', 'person'] ...\n",
      "cloning dataset quickstart to detectron-run-sampsa-quickstart-2022-10-04-22-17-06-604353\n",
      "instantiating Detectron2 predictor\n",
      "\n",
      "QUALITY PARAMETER 1\n",
      "/home/sampsa/silo/interdigital/venv_all/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 Evaluating detections...\n",
      " 100% |███████████| 2/2 [14.5ms elapsed, 0s remaining, 138.2 samples/s] \n",
      "Performing IoU sweep...\n",
      " 100% |███████████| 2/2 [22.8ms elapsed, 0s remaining, 87.7 samples/s] \n",
      "deleting tmp database detectron-run-sampsa-quickstart-2022-10-04-22-17-06-604353\n",
      "\n",
      "HAVE A NICE DAY!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision detectron2-eval --y --dataset-name=quickstart \\\n",
    "--slice=0:2 \\\n",
    "--gt-field=ground_truth \\\n",
    "--eval-method=coco \\\n",
    "--scale=100 \\\n",
    "--progressbar \\\n",
    "--qpars=1 \\\n",
    "--compressai-model-name=bmshj2018_factorized \\\n",
    "--output=compressai_detectron2_test.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb9a9f",
   "metadata": {},
   "source": [
    "Let's see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98d778e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"dataset\": \"quickstart\", \"gt_field\": \"ground_truth\", \"tmp datasetname\": \"detectron-run-sampsa-quickstart-2022-10-04-22-17-06-604353\", \"slice\": \"0:2\", \"model\": \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\", \"compressai model\": \"bmshj2018_factorized\", \"custom model\": null, \"checkpoint\": null, \"vtm\": false, \"vtm_cache\": null, \"qpars\": [1], \"bpp\": [0.18178251121076233], \"map\": [0.44477447744774484], \"map_per_class\": [{\"bird\": 0.100990099009901, \"horse\": 0.3333333333333334, \"person\": 0.9}]}"
     ]
    }
   ],
   "source": [
    "!cat compressai_detectron2_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e882ba7",
   "metadata": {},
   "source": [
    "Which is a single point on the mAP(bpp) curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c13d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
