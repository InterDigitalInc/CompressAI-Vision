{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce26285",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Tutorial, chapter 1\n",
    "\n",
    "- Sw stack check with \"info\"\n",
    "- downloading, listing\n",
    "- detectron2-eval for baseline & for qpoints using a demo slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb4c91d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:28:31.423650Z",
     "iopub.status.busy": "2022-10-10T19:28:31.422701Z",
     "iopub.status.idle": "2022-10-10T19:28:31.515600Z",
     "shell.execute_reply": "2022-10-10T19:28:31.514875Z"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45474/1348678174.py:6: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, Markdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://nbconvert.readthedocs.io/en/latest/removing_cells.html\n",
    "# use these magic spells to update your classes methods on-the-fly as you edit them:\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "# %run includeme.ipynb # include a notebook from this same directory\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2578782b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:28:31.518632Z",
     "iopub.status.busy": "2022-10-10T19:28:31.518278Z",
     "iopub.status.idle": "2022-10-10T19:28:49.742555Z",
     "shell.execute_reply": "2022-10-10T19:28:49.741484Z"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\r\n",
      "fiftyone imported\r\n",
      "\r\n",
      "removing tmp datasets for username sampsa\r\n",
      "WARNING: be sure not to remove datasets currently used by a process\r\n",
      "importing fiftyone\r\n",
      "fiftyone imported\r\n",
      "removing dataset quickstart-2-dummy from fiftyone\r\n",
      "could not deregister because of Dataset 'quickstart-2-dummy' not found\r\n",
      "importing fiftyone\r\n",
      "fiftyone imported\r\n",
      "removing dataset quickstart from fiftyone\r\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision clean --y\n",
    "!compressai-vision deregister --dataset-name quickstart-2-dummy --y\n",
    "!compressai-vision deregister --dataset-name quickstart --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644b827",
   "metadata": {},
   "source": [
    "In this tutorial chapter you will learn:\n",
    "\n",
    "- Checking the installed software stack with ``compressai-vision info``\n",
    "- Downloading datasets with ``compressai-vision download``\n",
    "- Evaluating datasets with ``compressai-vision detectron2-eval`` for creating mAP(bpp) curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7ec4a",
   "metadata": {},
   "source": [
    "The command line interface (cli) has all the functionality for evaluating your deep-learning compression algorithm against standardized benchmarks.\n",
    "\n",
    "The cli is accessed with the ``compressai-vision`` command that has several subcommands for handling datasets, evaluating your models with them and for generating plots.  In detail:\n",
    "\n",
    "- ``compressai-vision -h`` gives you a short description of all commands\n",
    "- ``compressai-vision manual`` shows you a more thorough description\n",
    "- ``compressai-vision subcommand -h`` gives a detailed description of a certain subcommand\n",
    "\n",
    "The very first subcommand you should try is ``info``.  It gives you information about the installed software stack, library versions and registered datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd69b871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:28:49.747259Z",
     "iopub.status.busy": "2022-10-10T19:28:49.746856Z",
     "iopub.status.idle": "2022-10-10T19:28:56.517421Z",
     "shell.execute_reply": "2022-10-10T19:28:56.516038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "*** TORCH, CUDA, DETECTRON2, COMPRESSAI ***\r\n",
      "torch version       : 1.9.1+cu102\r\n",
      "cuda version        : 10.2\r\n",
      "detectron2 version  : 0.6\r\n",
      "compressai version  : 1.2.0.dev0\r\n",
      "\r\n",
      "*** CHECKING GPU AVAILABILITY ***\r\n",
      "device              : cpu\r\n",
      "\r\n",
      "*** TESTING FFMPEG ***\r\n",
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\r\n",
      "built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\r\n",
      "configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\r\n",
      "libavutil      56. 31.100 / 56. 31.100\r\n",
      "libavcodec     58. 54.100 / 58. 54.100\r\n",
      "libavformat    58. 29.100 / 58. 29.100\r\n",
      "libavdevice    58.  8.100 / 58.  8.100\r\n",
      "libavfilter     7. 57.100 /  7. 57.100\r\n",
      "libavresample   4.  0.  0 /  4.  0.  0\r\n",
      "libswscale      5.  5.100 /  5.  5.100\r\n",
      "libswresample   3.  5.100 /  3.  5.100\r\n",
      "libpostproc    55.  5.100 / 55.  5.100\r\n",
      "\r\n",
      "NOTICE: Using mongodb managed by fiftyone\r\n",
      "Be sure not to have extra mongod server(s) running on your system\r\n",
      "importing fiftyone..\r\n",
      "..imported\r\n",
      "fiftyone version: 0.16.6\r\n",
      "\r\n",
      "*** DATABASE ***\r\n",
      "info about your connection:\r\n",
      "Database(MongoClient(host=['localhost:40083'], document_class=dict, tz_aware=False, connect=True, appname='fiftyone'), 'fiftyone')\r\n",
      "\r\n",
      "\r\n",
      "*** DATASETS ***\r\n",
      "datasets currently registered into fiftyone\r\n",
      "name, length, first sample path\r\n",
      "mpeg-vcm-detection, 5000, /home/sampsa/fiftyone/mpeg-vcm-detection/data\r\n",
      "mpeg-vcm-detection-dummy, 1, /home/sampsa/fiftyone/mpeg-vcm-detection/data\r\n",
      "mpeg-vcm-segmentation, 5000, /home/sampsa/fiftyone/mpeg-vcm-segmentation/data\r\n",
      "open-images-v6-validation, 8189, /home/sampsa/fiftyone/open-images-v6/validation/data\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e600757",
   "metadata": {},
   "source": [
    "Another basic command is ``list`` that just shows you the registered datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87abd13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:28:56.524873Z",
     "iopub.status.busy": "2022-10-10T19:28:56.523711Z",
     "iopub.status.idle": "2022-10-10T19:29:04.077618Z",
     "shell.execute_reply": "2022-10-10T19:29:04.076194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\r\n",
      "fiftyone imported\r\n",
      "\r\n",
      "datasets currently registered into fiftyone\r\n",
      "name, length, first sample path\r\n",
      "mpeg-vcm-detection, 5000, /home/sampsa/fiftyone/mpeg-vcm-detection/data\r\n",
      "mpeg-vcm-detection-dummy, 1, /home/sampsa/fiftyone/mpeg-vcm-detection/data\r\n",
      "mpeg-vcm-segmentation, 5000, /home/sampsa/fiftyone/mpeg-vcm-segmentation/data\r\n",
      "open-images-v6-validation, 8189, /home/sampsa/fiftyone/open-images-v6/validation/data\r\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f23dc4",
   "metadata": {},
   "source": [
    "Datasets can be registered to and deregistered from fiftyone using the ``register`` and ``deregister`` subcommands, and downloaded and registered directly from [fiftyone dataset zoo](https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/datasets.html#dataset-zoo-quickstart) with the ``download`` command.  Let's use ``download`` to get the \"quickstart\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff8adeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:29:04.083942Z",
     "iopub.status.busy": "2022-10-10T19:29:04.083463Z",
     "iopub.status.idle": "2022-10-10T19:29:14.885918Z",
     "shell.execute_reply": "2022-10-10T19:29:14.884312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\r\n",
      "fiftyone imported\r\n",
      "\r\n",
      "WARNING: downloading ALL images.  You might want to use the --lists option to download only certain images\r\n",
      "Using list files:     None\r\n",
      "Number of images:     ?\r\n",
      "Database name   :     quickstart\r\n",
      "Subname/split   :     None\r\n",
      "Target dir      :     None\r\n",
      "\r\n",
      "Dataset already downloaded\r\n",
      "Loading 'quickstart'\r\n",
      " 100% |███████| 200/200 [3.0s elapsed, 0s remaining, 52.1 samples/s]       \r\n",
      "Dataset 'quickstart' created\r\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision download --dataset-name=quickstart --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3c53a",
   "metadata": {},
   "source": [
    "Nice, we have ourselves a dataset to play with.  A note: the ``--y`` switch makes the command to run in non-interactive mode.  Let's take a closer look at the fields that the samples have in this datafield with ``show``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd2b8f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:29:14.893418Z",
     "iopub.status.busy": "2022-10-10T19:29:14.892888Z",
     "iopub.status.idle": "2022-10-10T19:29:21.268977Z",
     "shell.execute_reply": "2022-10-10T19:29:21.267384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\r\n",
      "fiftyone imported\r\n",
      "\r\n",
      "dataset info:\r\n",
      "Name:        quickstart\r\n",
      "Media type:  image\r\n",
      "Num samples: 200\r\n",
      "Persistent:  True\r\n",
      "Tags:        []\r\n",
      "Sample fields:\r\n",
      "    id:           fiftyone.core.fields.ObjectIdField\r\n",
      "    filepath:     fiftyone.core.fields.StringField\r\n",
      "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\r\n",
      "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\r\n",
      "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\r\n",
      "    uniqueness:   fiftyone.core.fields.FloatField\r\n",
      "    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\r\n",
      "\r\n",
      "test-loading first image from /home/sampsa/fiftyone/quickstart/data/000880.jpg\r\n",
      "loaded image with dimensions (480, 640, 3) ok\r\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision show --dataset-name=quickstart --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3e468",
   "metadata": {},
   "source": [
    "Some fields of interests in each sample: ``filepath`` fields have the path to the downloaded images, while ``ground_truth`` fields have the ground-truth bounding boxes (\"quickstart\" dataset is a demo subset of COCO).\n",
    "\n",
    "Next we'll crunch all the images in the dataset through a Detectron2 predictor and evaluate the results using the COCO evaluation protocol: as a result, we'll get a mAP accuracy for the Detectron2 model.  Note that we have to indicate the ground truth field with ``--gt-field=ground_truth``.  Option ``--slice=0:2`` takes only the first two samples from the dataset for this run: its only for debugging run, so please feel free to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9922d0fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:29:21.275591Z",
     "iopub.status.busy": "2022-10-10T19:29:21.274817Z",
     "iopub.status.idle": "2022-10-10T19:29:42.906986Z",
     "shell.execute_reply": "2022-10-10T19:29:42.905724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\r\n",
      "fiftyone imported\r\n",
      "WARNING: using a dataset slice instead of full dataset: SURE YOU WANT THIS?\r\n",
      "\r\n",
      "Using dataset          : quickstart\r\n",
      "Dataset tmp clone      : detectron-run-sampsa-quickstart-2022-10-10-22-29-27-260938\r\n",
      "Image scaling          : 100\r\n",
      "WARNING: Using slice   : 0:2\r\n",
      "Number of samples      : 2\r\n",
      "Torch device           : cpu\r\n",
      "Detectron2 model       : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\r\n",
      "Model was trained with : coco_2017_train\r\n",
      "** Evaluation without Encoding/Decoding **\r\n",
      "Ground truth data field name\r\n",
      "                       : ground_truth\r\n",
      "Eval. results will be saved to datafield\r\n",
      "                       : detectron-predictions\r\n",
      "Evaluation protocol    : coco\r\n",
      "Progressbar            : True\r\n",
      "WARNING: progressbar enabled --> disabling normal progress print\r\n",
      "Print progress         : 0\r\n",
      "Output file            : detectron2_test.json\r\n",
      "Peek model classes     :\r\n",
      "['airplane', 'apple', 'backpack', 'banana', 'baseball bat'] ...\r\n",
      "Peek dataset classes   :\r\n",
      "['bird', 'horse', 'person'] ...\r\n",
      "cloning dataset quickstart to detectron-run-sampsa-quickstart-2022-10-10-22-29-27-260938\r\n",
      "instantiating Detectron2 predictor\r\n",
      "/home/sampsa/silo/interdigital/venv_all/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 error: number of pixels sum < 1\r\n",
      "Evaluating detections...\r\n",
      " 100% |███████████| 2/2 [9.5ms elapsed, 0s remaining, 211.5 samples/s] \r\n",
      "Performing IoU sweep...\r\n",
      " 100% |███████████| 2/2 [12.2ms elapsed, 0s remaining, 163.9 samples/s] \r\n",
      "deleting tmp database detectron-run-sampsa-quickstart-2022-10-10-22-29-27-260938\r\n",
      "\r\n",
      "HAVE A NICE DAY!\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision detectron2-eval --y --dataset-name=quickstart \\\n",
    "--slice=0:2 \\\n",
    "--gt-field=ground_truth \\\n",
    "--eval-method=coco \\\n",
    "--progressbar \\\n",
    "--output=detectron2_test.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaea264",
   "metadata": {},
   "source": [
    "Let's see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8bd8a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:29:42.912614Z",
     "iopub.status.busy": "2022-10-10T19:29:42.911793Z",
     "iopub.status.idle": "2022-10-10T19:29:43.039444Z",
     "shell.execute_reply": "2022-10-10T19:29:43.038505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"dataset\": \"quickstart\",\r\n",
      "  \"gt_field\": \"ground_truth\",\r\n",
      "  \"tmp datasetname\": \"detectron-run-sampsa-quickstart-2022-10-10-22-29-27-260938\",\r\n",
      "  \"slice\": \"0:2\",\r\n",
      "  \"model\": \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\",\r\n",
      "  \"codec\": \"\",\r\n",
      "  \"qpars\": null,\r\n",
      "  \"bpp\": [\r\n",
      "    -1\r\n",
      "  ],\r\n",
      "  \"map\": [\r\n",
      "    0.5676567656765678\r\n",
      "  ],\r\n",
      "  \"map_per_class\": [\r\n",
      "    {\r\n",
      "      \"bird\": 0.30297029702970296,\r\n",
      "      \"horse\": 0.5,\r\n",
      "      \"person\": 0.9\r\n",
      "    }\r\n",
      "  ]\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat detectron2_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dba7f0",
   "metadata": {},
   "source": [
    "Now we use again a Detectron2 predictor on our dataset.   However, before passing the images to Detectron2 model, they are first compressed and decompressed by using a pre-trained compressai model with a quality parameter 1 (``--qpars=1``).\n",
    "\n",
    "We could evaluate for several quality parameters in serial by defining a list, i.e: ``--qpars=1,2,3`` and in parallel by launching the command separately for each particular value (say, for calculations in a queue/grid system).\n",
    "\n",
    "A scaling can be applied on the images, as defined by the mpeg-vcm specifications (``--scale=100``).  Again, remember to remove ``--slice=0:2`` for an actual run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49dfee98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:29:43.043628Z",
     "iopub.status.busy": "2022-10-10T19:29:43.043340Z",
     "iopub.status.idle": "2022-10-10T19:30:08.408843Z",
     "shell.execute_reply": "2022-10-10T19:30:08.407417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\r\n",
      "fiftyone imported\r\n",
      "WARNING: using a dataset slice instead of full dataset: SURE YOU WANT THIS?\r\n",
      "\r\n",
      "Using dataset          : quickstart\r\n",
      "Dataset tmp clone      : detectron-run-sampsa-quickstart-2022-10-10-22-29-49-246836\r\n",
      "Image scaling          : 100\r\n",
      "WARNING: Using slice   : 0:2\r\n",
      "Number of samples      : 2\r\n",
      "Torch device           : cpu\r\n",
      "Detectron2 model       : COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\r\n",
      "Model was trained with : coco_2017_train\r\n",
      "Using compressai model : bmshj2018_factorized\r\n",
      "Quality parameters     : [1]\r\n",
      "Ground truth data field name\r\n",
      "                       : ground_truth\r\n",
      "Eval. results will be saved to datafield\r\n",
      "                       : detectron-predictions\r\n",
      "Evaluation protocol    : coco\r\n",
      "Progressbar            : True\r\n",
      "WARNING: progressbar enabled --> disabling normal progress print\r\n",
      "Print progress         : 0\r\n",
      "Output file            : compressai_detectron2_test.json\r\n",
      "Peek model classes     :\r\n",
      "['airplane', 'apple', 'backpack', 'banana', 'baseball bat'] ...\r\n",
      "Peek dataset classes   :\r\n",
      "['bird', 'horse', 'person'] ...\r\n",
      "cloning dataset quickstart to detectron-run-sampsa-quickstart-2022-10-10-22-29-49-246836\r\n",
      "instantiating Detectron2 predictor\r\n",
      "\r\n",
      "QUALITY PARAMETER:  1\r\n",
      "/home/sampsa/silo/interdigital/venv_all/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\r\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\r\n",
      "  return torch.floor_divide(self, other)\r\n",
      " 100% |███████████████████████████████████████████████████████████████████| 2/2 Evaluating detections...\r\n",
      " 100% |███████████| 2/2 [21.9ms elapsed, 0s remaining, 91.5 samples/s] \r\n",
      "Performing IoU sweep...\r\n",
      " 100% |███████████| 2/2 [30.0ms elapsed, 0s remaining, 66.8 samples/s] \r\n",
      "deleting tmp database detectron-run-sampsa-quickstart-2022-10-10-22-29-49-246836\r\n",
      "\r\n",
      "HAVE A NICE DAY!\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision detectron2-eval --y --dataset-name=quickstart \\\n",
    "--slice=0:2 \\\n",
    "--gt-field=ground_truth \\\n",
    "--eval-method=coco \\\n",
    "--scale=100 \\\n",
    "--progressbar \\\n",
    "--qpars=1 \\\n",
    "--compressai-model-name=bmshj2018_factorized \\\n",
    "--output=compressai_detectron2_test.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb9a9f",
   "metadata": {},
   "source": [
    "Let's see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d778e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T19:30:08.414270Z",
     "iopub.status.busy": "2022-10-10T19:30:08.413870Z",
     "iopub.status.idle": "2022-10-10T19:30:08.542017Z",
     "shell.execute_reply": "2022-10-10T19:30:08.541273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"dataset\": \"quickstart\",\r\n",
      "  \"gt_field\": \"ground_truth\",\r\n",
      "  \"tmp datasetname\": \"detectron-run-sampsa-quickstart-2022-10-10-22-29-49-246836\",\r\n",
      "  \"slice\": \"0:2\",\r\n",
      "  \"model\": \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\",\r\n",
      "  \"codec\": \"bmshj2018_factorized\",\r\n",
      "  \"qpars\": [\r\n",
      "    1\r\n",
      "  ],\r\n",
      "  \"bpp\": [\r\n",
      "    0.18178251121076233\r\n",
      "  ],\r\n",
      "  \"map\": [\r\n",
      "    0.44477447744774484\r\n",
      "  ],\r\n",
      "  \"map_per_class\": [\r\n",
      "    {\r\n",
      "      \"bird\": 0.100990099009901,\r\n",
      "      \"horse\": 0.3333333333333334,\r\n",
      "      \"person\": 0.9\r\n",
      "    }\r\n",
      "  ]\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat compressai_detectron2_test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e882ba7",
   "metadata": {},
   "source": [
    "Which is a single point on the mAP(bpp) curve.  Next you need to produce some more points and then use ``plot`` subcommand.  An explicit example of that is given in the mpeg-vcm section of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c13d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
