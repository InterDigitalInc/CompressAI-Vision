{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28098583",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "# Tutorial, chapter 4\n",
    "\n",
    "- run mpeg-vcm evaluation for your development model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb280563",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40153/1348678174.py:6: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, Markdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://nbconvert.readthedocs.io/en/latest/removing_cells.html\n",
    "# use these magic spells to update your classes methods on-the-fly as you edit them:\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "# %run includeme.ipynb # include a notebook from this same directory\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ac77a",
   "metadata": {},
   "source": [
    "In this chapter you will learn:\n",
    "\n",
    "- to evaluate a custom model you're currently developing agains the mpeg-vcm tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74427a33",
   "metadata": {},
   "source": [
    "As in the previous chapters, let's first check we have the dataset ``oiv6-mpeg-detection-v1`` available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4f013b",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing fiftyone\n",
      "fiftyone imported\n",
      "\n",
      "datasets currently registered into fiftyone\n",
      "name, length, first sample path\n",
      "detectron-run-sampsa-oiv6-mpeg-detection-v1-2022-11-16-17-22-40-319395, 2, /home/sampsa/fiftyone/oiv6-mpeg-detection-v1/data\n",
      "detectron-run-sampsa-oiv6-mpeg-detection-v1-2022-11-16-17-24-14-478278, 2, /home/sampsa/fiftyone/oiv6-mpeg-detection-v1/data\n",
      "flir-image-rgb-v1, 10318, /media/sampsa/4d0dff98-8e61-4a0b-a97e-ceb6bc7ccb4b/datasets/flir/images_rgb_train/data\n",
      "oiv6-mpeg-detection-v1, 5000, /home/sampsa/fiftyone/oiv6-mpeg-detection-v1/data\n",
      "oiv6-mpeg-detection-v1-dummy, 1, /home/sampsa/fiftyone/oiv6-mpeg-detection-v1/data\n",
      "oiv6-mpeg-segmentation-v1, 5000, /home/sampsa/fiftyone/oiv6-mpeg-segmentation-v1/data\n",
      "open-images-v6-validation, 8189, /home/sampsa/fiftyone/open-images-v6/validation/data\n",
      "quickstart, 200, /home/sampsa/fiftyone/quickstart/data\n",
      "quickstart-video, 10, /home/sampsa/fiftyone/quickstart-video/data\n",
      "sfu-hw-objects-v1, 2, /home/sampsa/silo/interdigital/mock/SFU-HW-Objects-v1/ClassC/Annotations/BasketballDrill\n",
      "tvd-image-detection-v1, 167, /media/sampsa/4d0dff98-8e61-4a0b-a97e-ceb6bc7ccb4b/datasets/tvd/TVD_images_detection_v1/data\n",
      "tvd-image-segmentation-v1, 167, /media/sampsa/4d0dff98-8e61-4a0b-a97e-ceb6bc7ccb4b/datasets/tvd/TVD_images_segmentation_v1/data\n",
      "tvd-object-tracking-v1, 3, /media/sampsa/4d0dff98-8e61-4a0b-a97e-ceb6bc7ccb4b/datasets/tvd/TVD_object_tracking_dataset_and_annotations\n"
     ]
    }
   ],
   "source": [
    "!compressai-vision list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bdb41",
   "metadata": {},
   "source": [
    "In order for your custom model to work with compressai-vision, it needs to be in a separate folder.  The entry-point must be called ``model.py``.  We provide an example for this: please take a look at the [examples/models/bmshj2018-factorized/](https://github.com/InterDigitalInc/CompressAI-Vision/tree/main/examples/models/bmshj2018-factorized) folder, where you have the following files: \n",
    "```\n",
    "├── bmshj2018-factorized-prior-1-446d5c7f.pth.tar\n",
    "├── bmshj2018-factorized-prior-2-87279a02.pth.tar\n",
    "└── model.py\n",
    "```\n",
    "The ``.pth.tar`` files are the checkpoints of your model, while ``model.py`` contains the pytorch/compressai custom code of your model.\n",
    "\n",
    "The requirement for ``model.py`` is simple.  You need to define this function:\n",
    "```\n",
    "getEncoderDecoder(quality=None, **kwargs)\n",
    "```\n",
    "which returns a subclass of an ``EncoderDecoder`` instance.  ``EncoderDecoder``  objects know how to do just that: encode and decode an image, and returning the final decoded image with bits-per-pixel value.\n",
    "\n",
    "``quality`` should be an integer parameter (it will be used by the ``--qpars`` command-line flag).  The quality parameter is mapped to a certain model checkpoint file in ``model.py``:\n",
    "```\n",
    "qpoint_per_file = {\n",
    "    1 : \"bmshj2018-factorized-prior-1-446d5c7f.pth.tar\",\n",
    "    2 : \"bmshj2018-factorized-prior-2-87279a02.pth.tar\"\n",
    "}\n",
    "```\n",
    "i.e. if you define ``--qpars=1``, the model will use ``bmshj2018-factorized-prior-1-446d5c7f.pth.tar`` from the directory.\n",
    "\n",
    "As you can learn from the code, ``EncoderDecoder`` object uses an underlying (compressai-based) model to perform the actual encoding/decoding.\n",
    "\n",
    "The requirement for the model class (``class FactorizedPrior(CompressionModel)`` in the example model.py) are minimal: your model class should have two methods, called ``compress`` and ``decompress``.  Method ``compress`` takes in an RGB image tensor and returns bitstream, while ``decompress`` takes in bitstream and returns a recovered image.\n",
    "\n",
    "The exact signatures are:\n",
    "```\n",
    "def compress(self, x): -> dict\n",
    "    # where x is a torch RGB image tensor (batch, 3, H, W) \n",
    "    ...\n",
    "    return {\"strings\": STRINGS, \"shape\": SHAPE}\n",
    "    # STRINGS: a list where STRINGS[0][0] is a bytes object (the encoded bitstream) and SHAPE is some shape information used by your model\n",
    "    \n",
    "    \n",
    "def decompress(self, STRINGS, SHAPE): -> dict\n",
    "    # where STRINGS and SHAPE are the objects returned by compress\n",
    "    ...\n",
    "    return {\"x_hat\": x_hat}\n",
    "    # where x_hat is a torch RGB image tensor (batch, 3, H, W)\n",
    "```\n",
    "This signature/interface is used by the compressai library models.  When you have these same methods in *your* custom model, you can use the ``CompressAIEncoderDecoder`` straight out of the box with it.\n",
    "\n",
    "You can also implement your own ``EncoderDecoder`` class (say, for comparing results to classical codecs like jpeg, etc.).  For this, please refer to the example jpeg ``EncoderDecoder`` class in the library tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f80b0",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "source": [
    "So, take a copy of the ``examples/models/bmshj2018-factorized/`` folder into your disk and run:\n",
    "```\n",
    "compressai-vision detectron2-eval --y \\\n",
    "--dataset-name oiv6-mpeg-detection-v1 \\\n",
    "--slice=0:2 \\\n",
    "--scale=100 \\\n",
    "--gt-field=detections \\\n",
    "--eval-method=open-images \\\n",
    "--progressbar \\\n",
    "--compression-model-path /path/to/examples/models/bmshj2018-factorized/ \\\n",
    "--qpars=1,2 \\\n",
    "--output=detectron2_bmshj2018-factorized.json \\\n",
    "--model=COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
    "```\n",
    "This will evaluate your custom model with Detectron2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f530b8",
   "metadata": {},
   "source": [
    "Again, for an actual production run, you would remove the ``--slice`` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b0a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
