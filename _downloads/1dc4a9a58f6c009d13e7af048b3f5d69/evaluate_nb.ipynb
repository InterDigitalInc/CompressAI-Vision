{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b41d479",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "## 4. Evaluate\n",
    "\n",
    "- Run evaluation on fiftyone dataset with Detectron2 results\n",
    "- Show how to evaluate with VTM as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc06938b",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69474/3813857106.py:5: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, Markdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use these magic spells to update your classes methods on-the-fly as you edit them:\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "# %run includeme.ipynb # include a notebook from this same directory\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7ef99",
   "metadata": {},
   "source": [
    "In this tutorial we evaluate mAP values for a dataset with Detectron2 and a deep-learning encoding model from the CompressAI library.  We also show how to perform a baseline evaluation with VTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d171492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libs\n",
    "import math, os, io, json, cv2, random, logging, pickle, datetime\n",
    "import numpy as np\n",
    "# torch\n",
    "import torch\n",
    "# images\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# compressai\n",
    "from compressai.zoo import bmshj2018_factorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db77483",
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** Detectron imports ***\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e246d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CompressAI-Vision\n",
    "from compressai_vision.conversion import FO2DetectronDataset # convert fiftyone dataset to Detectron2 dataset\n",
    "from compressai_vision.conversion import detectron251 # convert Detectron2 results to fiftyone format\n",
    "from compressai_vision.evaluation.fo import annexPredictions # annex predictions from\n",
    "from compressai_vision.evaluation.pipeline import CompressAIEncoderDecoder, VTMEncoderDecoder # a class that does encoding+decoding & returns the transformed image & bitrate\n",
    "from compressai_vision.tools import confLogger, quickLog, getDataFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f386b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiftyone\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d503052c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9593bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL A\n",
    "model_name=\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"\n",
    "## look here:\n",
    "## https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#faster-r-cnn\n",
    "## for the line that says X101-FPN --> box AP is 43\n",
    "\n",
    "## MODEL B\n",
    "# model_name=\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a20652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected input colorspace: BGR\n",
      "loaded datasets: PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
      "PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
      "PROPOSAL_FILES_TEST: ()\n",
      "PROPOSAL_FILES_TRAIN: ()\n",
      "TEST: ('coco_2017_val',)\n",
      "TRAIN: ('coco_2017_train',)\n",
      "model was trained with coco_2017_train\n"
     ]
    }
   ],
   "source": [
    "# cfg encapsulates the model architecture & weights, also threshold parameter, metadata, etc.\n",
    "cfg = get_cfg()\n",
    "cfg.MODEL.DEVICE=device\n",
    "# load config from a file:\n",
    "cfg.merge_from_file(model_zoo.get_config_file(model_name))\n",
    "# DO NOT TOUCH THRESHOLD WHEN DOING EVALUATION:\n",
    "# too big a threshold will cut the smallest values & affect the precision(recall) curves & evaluation results\n",
    "# the default value is 0.05\n",
    "# value of 0.01 saturates the results (they don't change at lower values)\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "# get weights\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_name)\n",
    "print(\"expected input colorspace:\", cfg.INPUT.FORMAT)\n",
    "print(\"loaded datasets:\", cfg.DATASETS)\n",
    "model_dataset=cfg.DATASETS.TRAIN[0]\n",
    "print(\"model was trained with\", model_dataset)\n",
    "model_meta=MetadataCatalog.get(model_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e909bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_meta.thing_classes # check class labels this was trained with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab5cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ece99f",
   "metadata": {},
   "source": [
    "Get handle to a dataset.  We will be using the ``oiv6-mpeg-v1`` dataset.  Please go through the CLI Tutorials in order to produce this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf1def5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset(\"oiv6-mpeg-detection-v1-dummy\") # or use the dummy dataset for testing/debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbd34b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        oiv6-mpeg-detection-v1-dummy\n",
       "Media type:  image\n",
       "Num samples: 1\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:              fiftyone.core.fields.ObjectIdField\n",
       "    filepath:        fiftyone.core.fields.StringField\n",
       "    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    positive_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
       "    negative_labels: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classifications)\n",
       "    detections:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    open_images_id:  fiftyone.core.fields.StringField"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638769cf",
   "metadata": {},
   "source": [
    "Set some loglevels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed28678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger CompressAIEncoderDecoder (INFO)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loglev=logging.DEBUG\n",
    "loglev=logging.INFO\n",
    "quickLog(\"CompressAIEncoderDecoder\", loglev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c442f",
   "metadata": {},
   "source": [
    "Get a list of labels in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a9dc4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane']\n"
     ]
    }
   ],
   "source": [
    "classes = dataset.distinct(\n",
    "    \"detections.detections.label\"\n",
    ")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d58ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class(results_obj):\n",
    "    \"\"\"helper function: take fiftyone/openimagev6 results object & spit\n",
    "    out mAP breakdown as per class\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for class_ in classes:\n",
    "        d[class_] = results_obj.mAP([class_])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945360c",
   "metadata": {},
   "source": [
    "``CompressAIEncoderDecoder`` is a subclass of ``EncoderDecoder``, i.e. it's a class that encodes an image, decodes it, and returns the transformed (encoded+decoded) image and the bitrate of the encoded image.\n",
    "\n",
    "In particular ``CompressAIEncoderDecoder`` uses a CompressAI encoder/decoder to achieve this.\n",
    "\n",
    "You used ``annexPredictions`` in the previous notebook to push the dataset through a Detectron2 predictor.  Here, we provide it with an additional parameter: an ``EncoderDecoder`` class that transforms the image before the image is passed to the Detectron2 predictor.\n",
    "\n",
    "We run the ``bmshj2018_factorized`` model over various quality parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9ce7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[1] # debugging\n",
    "# params=[1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17947ef2",
   "metadata": {},
   "source": [
    "Detectron prediction results are saved during the run into the fiftyone (mongodb) database.  Let's define a unique name for the sample field where the detectron results are saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5db9adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_field='detectron-predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e84407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running the detector at 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sampsa/silo/interdigital/venv_all/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:  1 / 1\n",
      "Evaluating detections...\n",
      " 100% |█████████████████████| 1/1 [11.6ms elapsed, 0s remaining, 85.9 samples/s] \n",
      "ready!\n"
     ]
    }
   ],
   "source": [
    "xs=[]; ys=[]; maps=[]; # bpp, mAP values, mAP(s) per class\n",
    "results=[] # complete results\n",
    "for i in params:\n",
    "    net = bmshj2018_factorized(quality=i, pretrained=True).eval().to(device)\n",
    "    enc_dec = CompressAIEncoderDecoder(net, device=device)\n",
    "    # note the EncoderDecoder instance here:\n",
    "    # before the predictor is used, the image is crunched through the encoding/decoding process & the bitrate is recorded\n",
    "    # you could substitute CompressAIEncoderDecoder with VTMEncoderDecoder if you'd like to (see also the end of this tutorial)\n",
    "    print(\"running the detector at\", i)\n",
    "    bpp = annexPredictions(predictors=[predictor], fo_dataset=dataset, encoder_decoder=enc_dec, predictor_fields=[predictor_field])\n",
    "    # .. now detectron's results are in each sample at the \"detectron-predictions\"  field\n",
    "    res = dataset.evaluate_detections(\n",
    "        predictor_field,\n",
    "        gt_field=\"detections\",\n",
    "        method=\"open-images\",\n",
    "        pos_label_field=\"positive_labels\",\n",
    "        neg_label_field=\"negative_labels\",\n",
    "        expand_pred_hierarchy=False,\n",
    "        expand_gt_hierarchy=False\n",
    "    )\n",
    "    results.append((i, bpp, res))\n",
    "    # save to disk at each iteration as a backup just in case\n",
    "    xs.append(bpp)\n",
    "    ys.append(res.mAP())\n",
    "    maps.append(per_class(res))\n",
    "    with open(\"out.json\",\"w\") as f:\n",
    "        f.write(json.dumps({\n",
    "            \"bpp\" : xs, \n",
    "            \"map\" : ys,\n",
    "            \"map_per_class\" : maps\n",
    "            }, indent=2))\n",
    "print(\"ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0879934",
   "metadata": {},
   "source": [
    "After the evaluation we can (and should!) remove the detectron results from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57335fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.delete_sample_fields(predictor_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a9914",
   "metadata": {},
   "source": [
    "Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f7a329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bpp': [0.10060123042505593], 'map': [1.0], 'map_per_class': [{'airplane': 1.0}]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"out.json\",\"r\") as f:\n",
    "    res=json.load(f)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11491cf5",
   "metadata": {},
   "source": [
    "In that loop over quality parameters above, you can substitute the ``CompressAIEncoderDecoder`` with ``VTMEncoderDecoder``in order to produce the anchor/baseline results.  Let's first set some variables for the VTM program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceaf58b7",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "path_to_vtm_software=\"/home/sampsa/silo/interdigital/VVCSoftware_VTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "123bf61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: set path_to_vtm_software\n",
    "vtm_encoder_app=os.path.join(path_to_vtm_software, \"bin/EncoderAppStatic\")\n",
    "vtm_decoder_app=os.path.join(path_to_vtm_software, \"bin/DecoderAppStatic\")\n",
    "vtm_cfg=os.path.join(path_to_vtm_software, \"cfg/encoder_intra_vtm.cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46180813",
   "metadata": {},
   "source": [
    "If you'd want to see what the VTM is doing exactly, enable debugging output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d366617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loglev=logging.DEBUG\n",
    "# loglev=logging.INFO\n",
    "log=quickLog(\"VTMEncoderDecoder\", loglev) # VTMEncoderDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26df80b",
   "metadata": {},
   "source": [
    "At each quality parameter in the loop, instantiate an ``VTMEncoderDecoder`` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b5e99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VTMEncoderDecoder - WARNING - folder /tmp/bitstreams/100/47 exists already\n"
     ]
    }
   ],
   "source": [
    "enc_dec = VTMEncoderDecoder(\n",
    "    encoderApp=vtm_encoder_app,\n",
    "    decoderApp=vtm_decoder_app,\n",
    "    ffmpeg=\"ffmpeg\",\n",
    "    vtm_cfg=vtm_cfg,\n",
    "    qp=47,\n",
    "    cache=\"/tmp/bitstreams\",\n",
    "    scale=100,\n",
    "    warn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9089b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "104d1e3c0714c39a49e0363d53a772ca68ba2a5370285cfea1720d3aa41a3850"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
